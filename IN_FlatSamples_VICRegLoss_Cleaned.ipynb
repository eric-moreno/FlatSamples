{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeefce5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Imports basics\n",
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "import json\n",
    "import setGPU\n",
    "import sklearn\n",
    "import corner\n",
    "import scipy\n",
    "import time\n",
    "from tqdm import tqdm \n",
    "import math\n",
    "\n",
    "# Imports neural net tools\n",
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd.variable import *\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from fast_soft_sort.pytorch_ops import soft_rank\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, accuracy_score,  auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf55b46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Opens files and reads data\n",
    "\n",
    "print(\"Extracting\")\n",
    "fOne = h5py.File(\"data/FullQCD_FullSig_Zqq_noFill_dRlimit08_100particlesordered_genMatched50_ECF_flatratio_.h5\", 'r')\n",
    "totalData = fOne[\"deepDoubleQ\"][:]\n",
    "print(totalData.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378cd508",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sets controllable values\n",
    "\n",
    "particlesConsidered = 100\n",
    "particlesPostCut = 100\n",
    "entriesPerParticle = 4\n",
    "eventDataFeatures = ['jet_eta', 'jet_phi', 'jet_EhadOverEem', 'jet_mass', 'jet_pt', \n",
    "                 'jet_sdmass']#, 'ecfns_2_1', 'ecfns_3_2', 'N2']\n",
    "eventDataLength = len(eventDataFeatures)\n",
    "decayTypeColumn = -1\n",
    "trainingDataLength = int(len(totalData)*0.8)\n",
    "validationDataLength = int(len(totalData)*0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685318e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creates Training Data\n",
    "\n",
    "print(\"Preparing Data\")\n",
    "\n",
    "particleDataLength = particlesConsidered * entriesPerParticle\n",
    "\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(totalData)\n",
    "\n",
    "#trainingDataLength = int(datapoints*0.8)\n",
    "#validationDataLength = int(datapoints*0.1)\n",
    "\n",
    "#mask = [i>90 and i<110 for i in totalData[:, eventDataLength-1] ]\n",
    "#totalData = totalData[mask]\n",
    "\n",
    "\n",
    "peaky=False\n",
    "if peaky:\n",
    "    print('Creating peaky sample')\n",
    "    mask = []\n",
    "    msd_sig_min = 60\n",
    "    msd_sig_max = 100\n",
    "    for i in range(totalData.shape[0]):\n",
    "        if totalData[i,-1]==0:\n",
    "            mask.append(True)\n",
    "        else:\n",
    "            mass = totalData[i,5]\n",
    "            if (mass > msd_sig_min) and (mass < msd_sig_max):\n",
    "                mask.append(True)\n",
    "            else:\n",
    "                mask.append(False)\n",
    "\n",
    "    totalData = totalData[mask]\n",
    "    \n",
    "    trainingDataLength = int(len(totalData)*0.8)\n",
    "    validationDataLength = int(len(totalData)*0.1)\n",
    "\n",
    "labels = totalData[:, decayTypeColumn:]\n",
    "particleData = totalData[:, eventDataLength:particleDataLength + eventDataLength]\n",
    "eventData = totalData[:, :eventDataLength]\n",
    "jetMassData = totalData[:, 5] #last entry in eventData (zero indexing)\n",
    "\n",
    "\n",
    "######### Training Data ###############\n",
    "eventTrainingData = np.array(eventData[0:trainingDataLength])\n",
    "jetMassTrainingData = np.array(jetMassData[0:trainingDataLength])\n",
    "particleTrainingData = np.transpose(\n",
    "    particleData[0:trainingDataLength, ].reshape(trainingDataLength, \n",
    "                                                 entriesPerParticle, \n",
    "                                                 particlesConsidered),\n",
    "                                                 axes=(0, 1, 2))\n",
    "trainingLabels = np.array([[i, 1-i] for i in labels[0:trainingDataLength]]).reshape((-1, 2))\n",
    "print(particleTrainingData.shape)\n",
    "\n",
    "########## Validation Data ##########\n",
    "eventValidationData = np.array(eventData[trainingDataLength:trainingDataLength + validationDataLength])\n",
    "jetMassValidationData = np.array(jetMassData[trainingDataLength:trainingDataLength + validationDataLength])\n",
    "particleValidationData = np.transpose(\n",
    "    particleData[trainingDataLength:trainingDataLength + validationDataLength, ].reshape(validationDataLength,\n",
    "                                                                                         entriesPerParticle,\n",
    "                                                                                         particlesConsidered),\n",
    "                                                                                         axes=(0, 1, 2))\n",
    "validationLabels = np.array([[i, 1-i] for i in labels[trainingDataLength:trainingDataLength + validationDataLength]]).reshape((-1, 2))\n",
    "print(particleValidationData.shape)\n",
    "\n",
    "\n",
    "########### Testing Data ############\n",
    "eventTestData = np.array(eventData[trainingDataLength + validationDataLength:])\n",
    "jetMassTestData = np.array(jetMassData[trainingDataLength + validationDataLength:])\n",
    "particleTestData = np.transpose(particleData[trainingDataLength + validationDataLength:,].reshape(\n",
    "    len(particleData) - trainingDataLength - validationDataLength, entriesPerParticle, particlesConsidered),\n",
    "                                axes=(0, 1, 2))\n",
    "testLabels = np.array([[i, 1-i] for i in labels[trainingDataLength + validationDataLength:]]).reshape((-1, 2))\n",
    "\n",
    "print('Selecting particlesPostCut')\n",
    "particleTrainingData = particleTrainingData[:, :particlesPostCut]\n",
    "particleValidationData = particleValidationData[:, :particlesPostCut]\n",
    "particlesTestData = particleTestData[:, :particlesPostCut]\n",
    "\n",
    "particlesConsidered = particlesPostCut\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dbcdcf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Look at the data a bit!\n",
    "# Jet mass for correlation\n",
    "jetMassTrainingDataSig = jetMassTrainingData[trainingLabels[:,0].astype(bool)]\n",
    "jetMassTrainingDataBkg = jetMassTrainingData[trainingLabels[:,1].astype(bool)]\n",
    "\n",
    "print(jetMassTrainingDataSig.shape)\n",
    "print(jetMassTrainingDataBkg.shape)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(jetMassTrainingDataSig, alpha=0.5, bins=np.linspace(40, 250, 20), density = True)\n",
    "plt.hist(jetMassTrainingDataBkg, alpha=0.5, bins=np.linspace(40, 250, 20), density = True)\n",
    "#plt.hist(jetMassValidationDataSig, density=True, alpha=0.5)\n",
    "#plt.hist(jetMassValidationDataBkg, density=True, alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "ax.hist(jetMassTrainingDataSig,color=\"r\",bins=np.linspace(40,250,30),alpha=0.5,label=f\"Z' {len(jetMassTrainingDataSig)}\",density=True)\n",
    "ax.hist(jetMassTrainingDataBkg,color=\"b\",bins=np.linspace(40,250,30),alpha=0.5,label=f\"QCD {len(jetMassTrainingDataBkg)}\",density=True)\n",
    "plt.legend(loc=\"best\")\n",
    "ax.set_xlabel(\"Jet mass (GeV))\")\n",
    "ax.set_ylabel(\"Counts\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4103a733",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Defines the interaction matrices\n",
    "class GraphNetnoSV(nn.Module):\n",
    "    def __init__(self, n_constituents, n_targets, params, hidden, De=5, Do=6, dropout=0.1, \n",
    "                 softmax=False, attention_flag=False):\n",
    "        super(GraphNetnoSV, self).__init__()\n",
    "        self.hidden = int(hidden)\n",
    "        self.P = params\n",
    "        self.Nv = 0 \n",
    "        self.N = n_constituents\n",
    "        self.Nr = self.N * (self.N - 1)\n",
    "        self.Nt = self.N * self.Nv\n",
    "        self.Ns = self.Nv * (self.Nv - 1)\n",
    "        self.Dr = 0\n",
    "        self.De = De\n",
    "        self.Dx = 0\n",
    "        self.Do = Do\n",
    "        self.S = 0\n",
    "        self.n_targets = n_targets\n",
    "        self.assign_matrices()\n",
    "        self.softmax = softmax\n",
    "        self.attention_flag = attention_flag \n",
    "        \n",
    "        self.Ra = torch.ones(self.Dr, self.Nr)\n",
    "        self.fr1 = nn.Linear(2 * self.P + self.Dr, self.hidden).cuda()\n",
    "        self.fr2 = nn.Linear(self.hidden, int(self.hidden/2)).cuda()\n",
    "        self.fr3 = nn.Linear(int(self.hidden/2), self.De).cuda()\n",
    "        self.fr1_pv = nn.Linear(self.S + self.P + self.Dr, self.hidden).cuda()\n",
    "        self.fr2_pv = nn.Linear(self.hidden, int(self.hidden/2)).cuda()\n",
    "        self.fr3_pv = nn.Linear(int(self.hidden/2), self.De).cuda()\n",
    "        \n",
    "        self.fo1 = nn.Linear(self.P + self.Dx + (self.De), self.hidden).cuda()\n",
    "        self.fo2 = nn.Linear(self.hidden, int(self.hidden/2)).cuda()\n",
    "        self.fo3 = nn.Linear(int(self.hidden/2), self.Do).cuda()\n",
    "        \n",
    "        # Attention stuff\n",
    "        if attention_flag: \n",
    "            self.attention = nn.MultiheadAttention(embed_dim=Do, num_heads=int(Do/2), batch_first=True).cuda()\n",
    "            self.layer_norm_1 = nn.LayerNorm(Do).cuda()\n",
    "            self.layer_norm_2 = nn.LayerNorm(Do).cuda()\n",
    "            self.dropout_1 = nn.Dropout(dropout).cuda()\n",
    "            self.dropout_2 = nn.Dropout(dropout).cuda()\n",
    "            self.dropout_3 = nn.Dropout(dropout).cuda()\n",
    "            self.linear_1 = nn.Linear(Do, Do*2).cuda()\n",
    "            self.linear_2 = nn.Linear(Do*2, Do).cuda()\n",
    "            self.linear_3 = nn.Linear(Do*self.N, Do).cuda()\n",
    "            \n",
    "        self.fc_fixed = nn.Linear(self.Do, self.n_targets).cuda()\n",
    "            \n",
    "    def assign_matrices(self):\n",
    "        self.Rr = torch.zeros(self.N, self.Nr)\n",
    "        self.Rs = torch.zeros(self.N, self.Nr)\n",
    "        receiver_sender_list = [i for i in itertools.product(range(self.N), range(self.N)) if i[0]!=i[1]]\n",
    "        for i, (r, s) in enumerate(receiver_sender_list):\n",
    "            self.Rr[r, i] = 1\n",
    "            self.Rs[s, i] = 1\n",
    "        self.Rr = (self.Rr).cuda()\n",
    "        self.Rs = (self.Rs).cuda()\n",
    "\n",
    "    def forward(self, x):\n",
    "        ###PF Candidate - PF Candidate###\n",
    "        Orr = self.tmul(x, self.Rr)\n",
    "        Ors = self.tmul(x, self.Rs)\n",
    "        B = torch.cat([Orr, Ors], 1)\n",
    "        ### First MLP ###\n",
    "        B = torch.transpose(B, 1, 2).contiguous()\n",
    "        B = nn.functional.relu(self.fr1(B.view(-1, 2 * self.P + self.Dr)))\n",
    "        B = nn.functional.relu(self.fr2(B))\n",
    "        E = nn.functional.relu(self.fr3(B).view(-1, self.Nr, self.De))\n",
    "        del B\n",
    "        E = torch.transpose(E, 1, 2).contiguous()\n",
    "        Ebar_pp = self.tmul(E, torch.transpose(self.Rr, 0, 1).contiguous())\n",
    "        del E\n",
    "        \n",
    "\n",
    "        ####Final output matrix for particles###\n",
    "        C = torch.cat([x, Ebar_pp], 1)\n",
    "        del Ebar_pp\n",
    "        C = torch.transpose(C, 1, 2).contiguous()\n",
    "        ### Second MLP ###\n",
    "        C = nn.functional.relu(self.fo1(C.view(-1, self.P + self.Dx + (self.De))))\n",
    "        C = nn.functional.relu(self.fo2(C))\n",
    "        O = nn.functional.relu(self.fo3(C).view(-1, self.N, self.Do))\n",
    "        del C\n",
    "\n",
    "        \n",
    "        #Taking the sum of over each particle/vertex\n",
    "        if self.attention_flag: \n",
    "            O_norm = self.layer_norm_1(O)\n",
    "            N = O_norm + self.dropout_1(self.attention(O, O, O, need_weights=False)[0])\n",
    "            del O_norm\n",
    "            N2 = self.layer_norm_2(N)\n",
    "            N = N + self.dropout_3(self.linear_2(self.dropout_2(nn.ReLU()(self.linear_1(N2)))))\n",
    "            del N2\n",
    "            N = self.linear_3(torch.flatten(N,start_dim=1))\n",
    "        else: \n",
    "            N = torch.sum(O, dim=1)\n",
    "        \n",
    "        del O\n",
    "        ### Classification MLP ###\n",
    "\n",
    "        N = self.fc_fixed(N)\n",
    "        \n",
    "        if self.softmax:\n",
    "            N = nn.Softmax(dim=1)(N)\n",
    "        \n",
    "        return N\n",
    "            \n",
    "    def tmul(self, x, y):  #Takes (I * J * K)(K * L) -> I * J * L \n",
    "        x_shape = x.size()\n",
    "        y_shape = y.size()\n",
    "        return torch.mm(x.view(-1, x_shape[2]), y).view(-1, x_shape[1], y_shape[1])\n",
    "\n",
    "class GraphNetv2(nn.Module):\n",
    "    def __init__(self, n_constituents, n_targets, params, hidden=40, De=80, Do=15, dropout=0.1, \n",
    "                 softmax=False, attention_flag=False):\n",
    "        super(GraphNetv2, self).__init__()\n",
    "        self.hidden = int(hidden)\n",
    "        self.P = params\n",
    "        self.Nv = 0 \n",
    "        self.N = n_constituents\n",
    "        self.Nr = self.N * (self.N - 1)\n",
    "        self.Nt = self.N * self.Nv\n",
    "        self.Ns = self.Nv * (self.Nv - 1)\n",
    "        self.De = De\n",
    "        self.Do = Do\n",
    "        self.n_targets = n_targets\n",
    "        self.assign_matrices()\n",
    "        self.softmax = softmax\n",
    "        self.relu = nn.ReLU()\n",
    "        self.attention_flag = attention_flag \n",
    "        \n",
    "\n",
    "        self.fr1 = nn.Conv1d(2*self.P, self.De, kernel_size=1).cuda()\n",
    "        self.fr2 = nn.Conv1d(self.De, int(self.De/2), kernel_size=1).cuda()\n",
    "        self.fr3 = nn.Conv1d(int(self.De/2), int(self.De/4), kernel_size=1).cuda()\n",
    "        self.fr_batchnorm = nn.BatchNorm1d(int(self.De/4),  momentum=0.6).cuda()\n",
    "        \n",
    "        self.fo1 = nn.Conv1d(self.P + self.Do, self.hidden, kernel_size=1).cuda()\n",
    "        self.fo2 = nn.Conv1d(self.hidden, int(self.hidden/2), kernel_size=1).cuda()\n",
    "        self.fo3 = nn.Conv1d(int(self.hidden/2), self.Do, kernel_size=1).cuda()\n",
    "        \n",
    "        # Attention stuff\n",
    "        if attention_flag: \n",
    "            self.attention = nn.MultiheadAttention(embed_dim=Do, num_heads=int(Do/2), batch_first=True).cuda()\n",
    "            self.layer_norm_1 = nn.LayerNorm(Do).cuda()\n",
    "            self.layer_norm_2 = nn.LayerNorm(Do).cuda()\n",
    "            self.dropout_1 = nn.Dropout(dropout).cuda()\n",
    "            self.dropout_2 = nn.Dropout(dropout).cuda()\n",
    "            self.dropout_3 = nn.Dropout(dropout).cuda()\n",
    "            self.linear_1 = nn.Linear(Do, Do*2).cuda()\n",
    "            self.linear_2 = nn.Linear(Do*2, Do).cuda()\n",
    "            self.linear_3 = nn.Linear(Do*self.N, Do).cuda()\n",
    "            \n",
    "        self.fc_fixed = nn.Linear(self.Do, self.n_targets).cuda()\n",
    "            \n",
    "    def assign_matrices(self):\n",
    "        self.Rr = torch.zeros(self.N, self.Nr)\n",
    "        self.Rs = torch.zeros(self.N, self.Nr)\n",
    "        receiver_sender_list = [i for i in itertools.product(range(self.N), range(self.N)) if i[0]!=i[1]]\n",
    "        for i, (r, s) in enumerate(receiver_sender_list):\n",
    "            self.Rr[r, i] = 1\n",
    "            self.Rs[s, i] = 1\n",
    "        self.Rr = (self.Rr).cuda()\n",
    "        self.Rs = (self.Rs).cuda()\n",
    "\n",
    "    def forward(self, x):\n",
    "        ###PF Candidate - PF Candidate###\n",
    "        Orr = self.tmul(x, self.Rr)\n",
    "        Ors = self.tmul(x, self.Rs)\n",
    "        B = torch.cat([Orr, Ors], 1)\n",
    "        ### First MLP ###\n",
    "        #B = torch.transpose(B, 1, 2).contiguous()\n",
    "        B = nn.functional.relu(self.fr1(B))\n",
    "        B = nn.functional.relu(self.fr2(B))\n",
    "        E = nn.functional.relu(self.fr3(B))\n",
    "        E = self.fr_batchnorm(E) \n",
    "        del B\n",
    "        #E = torch.transpose(E, 2, 1).contiguous()\n",
    "        Ebar_pp = self.tmul(E, torch.transpose(self.Rr, 0, 1).contiguous())\n",
    "        del E\n",
    "    \n",
    "        ####Final output matrix for particles###\n",
    "        C = torch.cat([x, Ebar_pp], 1)\n",
    "        del Ebar_pp; torch.cuda.empty_cache()\n",
    "        #C = torch.transpose(C, 2, 1).contiguous()\n",
    "        ### Second MLP ###\n",
    "        C = nn.functional.relu(self.fo1(C))\n",
    "        C = nn.functional.relu(self.fo2(C))\n",
    "        O = nn.functional.relu(self.fo3(C))\n",
    "        del C\n",
    "        O = torch.transpose(O, 1, 2).contiguous()\n",
    "        \n",
    "        #Taking the sum of over each particle/vertex\n",
    "        if self.attention_flag: \n",
    "            O_norm = self.layer_norm_1(O)\n",
    "            N = O_norm + self.dropout_1(self.attention(O, O, O, need_weights=False)[0])\n",
    "            del O_norm\n",
    "            N2 = self.layer_norm_2(N)\n",
    "            N = N + self.dropout_3(self.linear_2(self.dropout_2(nn.ReLU()(self.linear_1(N2)))))\n",
    "            del N2\n",
    "            N = self.linear_3(torch.flatten(N,start_dim=1))\n",
    "        else: \n",
    "            N = torch.sum(O, dim=1)\n",
    "        \n",
    "        del O\n",
    "        \n",
    "        ### Classification MLP ###\n",
    "        N = self.fc_fixed(N)\n",
    "        \n",
    "        if self.softmax:\n",
    "            N = nn.Softmax(dim=1)(N)\n",
    "    \n",
    "        return N\n",
    "        del N; torch.cuda.empty_cache()\n",
    "            \n",
    "    def tmul(self, x, y):  #Takes (I * J * K)(K * L) -> I * J * L \n",
    "        x_shape = x.size()\n",
    "        y_shape = y.size()\n",
    "        return torch.mm(x.view(-1, x_shape[2]), y).view(-1, x_shape[1], y_shape[1])\n",
    "\n",
    "\n",
    "    \n",
    "class DNN(nn.Module):\n",
    "    def __init__(self, n_DimLatent, softmaxFlag=False):\n",
    "        super(DNN, self).__init__()\n",
    "        #self.flat = torch.flatten()\n",
    "        self.f0 = nn.Linear(200, 400).cuda()\n",
    "        self.f0b = nn.Linear(400, 400).cuda()\n",
    "        self.f1 = nn.Linear(400, 100).cuda()\n",
    "        self.f2 = nn.Linear(100, 50).cuda()\n",
    "        self.f3 = nn.Linear(50, 10).cuda()\n",
    "        self.f4 = nn.Linear(10, n_DimLatent).cuda()\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "        self.softmaxFlag = softmaxFlag\n",
    "    def forward(self, x): \n",
    "        x = torch.flatten(x,start_dim=1)\n",
    "        x = self.activation(self.f0(x))\n",
    "        x = self.activation(self.f0b(x))\n",
    "        x = self.activation(self.f1(x))\n",
    "        x = self.activation(self.f2(x))\n",
    "        x = self.activation(self.f3(x))\n",
    "        x = self.f4(x)\n",
    "        if self.softmaxFlag: return self.softmax(x)\n",
    "        else: return(x)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_inputs, n_targets):\n",
    "        super(MLP, self).__init__()\n",
    "        self.f1 = nn.Linear(n_inputs, n_inputs).cuda()\n",
    "        self.f2 = nn.Linear(n_inputs, int(n_inputs/2)).cuda()\n",
    "        self.f3 = nn.Linear(int(n_inputs/2), int(n_inputs/10)).cuda()\n",
    "        self.f4 = nn.Linear(int(n_inputs/10), n_targets).cuda()\n",
    "        self.activation = torch.nn.Softmax()\n",
    "    def forward(self, x): \n",
    "        x = torch.flatten(x,start_dim=1)\n",
    "        x = self.f1(x)\n",
    "        x = self.f2(x)\n",
    "        x = self.f3(x)\n",
    "        x = self.f4(x)\n",
    "        return(self.activation(x))\n",
    "    \n",
    "class Linear(nn.Module):\n",
    "    def __init__(self, n_inputs, n_targets):\n",
    "        super(Linear, self).__init__()\n",
    "        self.f1 = nn.Linear(n_inputs, n_targets).cuda()\n",
    "        self.activation = torch.nn.Sigmoid()\n",
    "    def forward(self, x): \n",
    "        x = self.f1(x)\n",
    "        return(self.activation(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28b20fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define losses \n",
    "class BarlowTwinsLoss(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, lambda_param=5e-3):\n",
    "        super(BarlowTwinsLoss, self).__init__()\n",
    "        self.lambda_param = lambda_param\n",
    "        self.device = torch.device('cuda:0')\n",
    "\n",
    "    def forward(self, z_a: torch.Tensor, z_b: torch.Tensor):\n",
    "        #self.device = (torch.device('cuda')if z_a.is_cuda else torch.device('cpu'))\n",
    "        # normalize repr. along the batch dimension\n",
    "        z_a_norm = (z_a - z_a.mean(0)) / z_a.std(0) # NxD\n",
    "        z_b_norm = (z_b - z_b.mean(0)) / z_b.std(0) # NxD\n",
    "\n",
    "        N = z_a.size(0)\n",
    "        D = z_a.size(1)\n",
    "\n",
    "        # cross-correlation matrix\n",
    "        c = torch.mm(z_a_norm.T, z_b_norm) / N # DxD\n",
    "        # loss\n",
    "        c_diff = (c - torch.eye(D, device=self.device)).pow(2) # DxD\n",
    "        # multiply off-diagonal elems of c_diff by lambda\n",
    "        c_diff[~torch.eye(D, dtype=bool)] *= self.lambda_param\n",
    "        loss = c_diff.sum()\n",
    "        return loss\n",
    "    \n",
    "# return a flattened view of the off-diagonal elements of a square matrix\n",
    "def off_diagonal(x):\n",
    "    n, m = x.shape\n",
    "    assert n == m\n",
    "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
    "\n",
    "class VICRegLoss(torch.nn.Module):\n",
    "    def __init__(self, lambda_param=1,mu_param=1,nu_param=20, sort_tolerance=1.0,sort_reg='l2'):\n",
    "        super(VICRegLoss, self).__init__()\n",
    "        self.lambda_param = lambda_param\n",
    "        self.mu_param = mu_param\n",
    "        self.nu_param = nu_param\n",
    "        self.tolerance = sort_tolerance\n",
    "        self.reg       = sort_reg\n",
    "        #self.device = torch.device('cpu')\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        #self.device = (torch.device('cuda') if x.is_cuda else torch.device('cpu'))\n",
    "        \n",
    "        #x_scale = x\n",
    "        #y_scale = y\n",
    "        repr_loss = F.mse_loss(x, y)\n",
    "        \n",
    "        #x = torch.cat(FullGatherLayer.apply(x), dim=0)\n",
    "        #y = torch.cat(FullGatherLayer.apply(y), dim=0)\n",
    "        x = x - x.mean(dim=0)\n",
    "        y = y - y.mean(dim=0)\n",
    "        N = x.size(0)\n",
    "        D = x.size(1)\n",
    "        \n",
    "        std_x = torch.sqrt(x.var(dim=0) + 0.0001)\n",
    "        std_y = torch.sqrt(y.var(dim=0) + 0.0001)\n",
    "        std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
    "\n",
    "        #x = torchsort.soft_rank(x.cpu(),regularization=self.reg,regularization_strength=self.tolerance,)\n",
    "        #y = torchsort.soft_rank(y.cpu(),regularization=self.reg,regularization_strength=self.tolerance,)\n",
    "        #x = x.cuda()\n",
    "        #y = y.cuda()\n",
    "        x = (x-x.mean(dim=0))/x.std(dim=0)\n",
    "        y = (y-y.mean(dim=0))/y.std(dim=0)\n",
    "        #x_scale = x_scale/x_scale.norm()\n",
    "        #y_scale = y_scale/y_scale.norm()\n",
    "        #z_a_norm = (z_a - z_a.mean(0)) / z_a.std(0) # NxD\n",
    "        #z_b_norm = (z_b - z_b.mean(0)) / z_b.std(0) # NxD\n",
    "\n",
    "        cov_x = (x.T @ x) / (N - 1)\n",
    "        cov_y = (y.T @ y) / (N - 1)\n",
    "        cov_loss = self.off_diagonal(cov_x).pow_(2).sum().div(D) + self.off_diagonal(cov_y).pow_(2).sum().div(D)\n",
    "        return repr_loss,cov_loss,std_loss\n",
    "    \n",
    "    def forward_old(self, x, y):\n",
    "        self.device = (torch.device('cuda')if x.is_cuda else torch.device('cpu'))\n",
    "        \n",
    "        x_scale = x\n",
    "        y_scale = y\n",
    "        repr_loss = F.mse_loss(x_scale, y_scale)\n",
    "        \n",
    "        #x = torch.cat(FullGatherLayer.apply(x), dim=0)\n",
    "        #y = torch.cat(FullGatherLayer.apply(y), dim=0)\n",
    "        x_scale = x_scale - x_scale.mean(dim=0)\n",
    "        y_scale = y_scale - y_scale.mean(dim=0)\n",
    "        #x = x_scale - x_scale.mean(dim=0)\n",
    "        #y = y_scale - y_scale.mean(dim=0)\n",
    "        N = x_scale.size(0)\n",
    "        D = x_scale.size(1)\n",
    "        \n",
    "        std_x = torch.sqrt(x_scale.var(dim=0) + 0.0001)\n",
    "        std_y = torch.sqrt(y_scale.var(dim=0) + 0.0001)\n",
    "        \n",
    "        x_scale = x_scale/std_x\n",
    "        y_scale = y_scale/std_y\n",
    "        \n",
    "        std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
    "\n",
    "        cov_x = (x_scale.T @ x_scale) / (N - 1)\n",
    "        cov_y = (y_scale.T @ y_scale) / (N - 1)\n",
    "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(D) + off_diagonal(cov_y).pow_(2).sum().div(D)\n",
    "\n",
    "        #loss = (self.lambda_param * repr_loss + self.mu_param * std_loss+ self.nu_param * cov_loss)\n",
    "        #print(repr_loss,cov_loss,std_loss)\n",
    "        return repr_loss,cov_loss,std_loss\n",
    "    \n",
    "    def off_diagonal(self,x):\n",
    "        n, m = x.shape\n",
    "        assert n == m\n",
    "        return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
    "    \n",
    "class CorrLoss(nn.Module):\n",
    "    def __init__(self, corr=False,sort_tolerance=1.0,sort_reg='l2'):\n",
    "        super(CorrLoss, self).__init__()\n",
    "        self.tolerance = sort_tolerance\n",
    "        self.reg       = sort_reg\n",
    "        self.corr      = corr\n",
    "        \n",
    "    def spearman(self, pred, target):\n",
    "        pred   = soft_rank(pred.cpu().reshape(1,-1),regularization=self.reg,regularization_strength=self.tolerance,)\n",
    "        target = soft_rank(target.cpu().reshape(1,-1),regularization=self.reg,regularization_strength=self.tolerance,)\n",
    "        #pred   = torchsort.soft_rank(pred.reshape(1,-1),regularization_strength=x)\n",
    "        #target = torchsort.soft_rank(target.reshape(1,-1),regularization_strength=x)\n",
    "        pred = pred - pred.mean()\n",
    "        pred = pred / pred.norm()\n",
    "        target = target - target.mean()\n",
    "        target = target / target.norm()\n",
    "        ret = (pred * target).sum()\n",
    "        if self.corr:\n",
    "            return (1-ret)*(1-ret)\n",
    "        else:\n",
    "            return ret*ret \n",
    "    \n",
    "    def forward(self, features, labels):\n",
    "        return self.spearman(features,labels)\n",
    "    \n",
    "\n",
    "class DiscoCorr(nn.Module):\n",
    "    def __init__(self,background_only=False,anti=False,background_label=1,power=2):\n",
    "        self.backonly = background_only\n",
    "        self.background_label = background_label\n",
    "        self.power = power\n",
    "        self.anti = anti\n",
    "\n",
    "    def distance_corr(self,var_1,var_2,normedweight,power=1):\n",
    "        xx = var_1.view(-1, 1).repeat(1, len(var_1)).view(len(var_1),len(var_1))\n",
    "        yy = var_1.repeat(len(var_1),1).view(len(var_1),len(var_1))\n",
    "        amat = (xx-yy).abs()\n",
    "        del xx,yy\n",
    "\n",
    "        amatavg = torch.mean(amat*normedweight,dim=1)\n",
    "        Amat=amat-amatavg.repeat(len(var_1),1).view(len(var_1),len(var_1))-amatavg.view(-1, 1).repeat(1, len(var_1)).view(len(var_1),len(var_1))+torch.mean(amatavg*normedweight)\n",
    "        del amat\n",
    "\n",
    "        xx = var_2.view(-1, 1).repeat(1, len(var_2)).view(len(var_2),len(var_2))\n",
    "        yy = var_2.repeat(len(var_2),1).view(len(var_2),len(var_2))\n",
    "        bmat = (xx-yy).abs()\n",
    "        del xx,yy\n",
    "\n",
    "        bmatavg = torch.mean(bmat*normedweight,dim=1)\n",
    "        Bmat=bmat-bmatavg.repeat(len(var_2),1).view(len(var_2),len(var_2))\\\n",
    "          -bmatavg.view(-1, 1).repeat(1, len(var_2)).view(len(var_2),len(var_2))\\\n",
    "          +torch.mean(bmatavg*normedweight)\n",
    "        del bmat\n",
    "\n",
    "        ABavg = torch.mean(Amat*Bmat*normedweight,dim=1)\n",
    "        AAavg = torch.mean(Amat*Amat*normedweight,dim=1)\n",
    "        BBavg = torch.mean(Bmat*Bmat*normedweight,dim=1)\n",
    "        del Bmat, Amat\n",
    "\n",
    "        if(power==1):\n",
    "            dCorr=(torch.mean(ABavg*normedweight))/torch.sqrt((torch.mean(AAavg*normedweight)*torch.mean(BBavg*normedweight)))\n",
    "        elif(power==2):\n",
    "            dCorr=(torch.mean(ABavg*normedweight))**2/(torch.mean(AAavg*normedweight)*torch.mean(BBavg*normedweight))\n",
    "        else:\n",
    "            dCorr=((torch.mean(ABavg*normedweight))/torch.sqrt((torch.mean(AAavg*normedweight)*torch.mean(BBavg*normedweight))))**power\n",
    "\n",
    "        return dCorr\n",
    "\n",
    "    def __call__(self,pred,x_biased,weights=None):\n",
    "        xweights = torch.ones_like(pred)\n",
    "        disco = self.distance_corr(x_biased,pred,normedweight=xweights,power=self.power)\n",
    "        if self.anti:\n",
    "            disco = 1-disco\n",
    "        return disco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc6cdf3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Separate Encoder & Classifier training \n",
    "def round_even(num):\n",
    "    return round(num / 2) * 2\n",
    "\n",
    "def train_encoder(encoder, batchSize, n_Dim, CorrDim, n_epochs, modelName, outdir,\n",
    "                  particleTrainingData, particleValidationData, trainingLabels, jetMassTrainingData, jetMassValidationData,\n",
    "                  weightrepr = 1, weightcov = 1, weightstd = 1, weightCorr1 = 0, weightCorr2 = 0):\n",
    "    \n",
    "    # Separating signal and bkg arrays\n",
    "    particleTrainingDataSig = particleTrainingData[trainingLabels[:,0].astype(bool)]\n",
    "    particleTrainingDataBkg = particleTrainingData[trainingLabels[:,1].astype(bool)]\n",
    "    particleValidationDataSig = particleValidationData[validationLabels[:,0].astype(bool)]\n",
    "    particleValidationDataBkg = particleValidationData[validationLabels[:,1].astype(bool)]\n",
    "    particleTrainingLabelSig = trainingLabels[trainingLabels[:,0].astype(bool)]\n",
    "    particleTrainingLabelBkg = trainingLabels[trainingLabels[:,1].astype(bool)]\n",
    "\n",
    "    # Jet mass for correlation\n",
    "    jetMassTrainingDataSig = jetMassTrainingData[trainingLabels[:,0].astype(bool)]\n",
    "    jetMassTrainingDataBkg = jetMassTrainingData[trainingLabels[:,1].astype(bool)]\n",
    "    jetMassValidationDataSig = jetMassValidationData[validationLabels[:,0].astype(bool)]\n",
    "    jetMassValidationDataBkg = jetMassValidationData[validationLabels[:,1].astype(bool)]\n",
    "        \n",
    "    try: \n",
    "        os.mkdir(outdir) \n",
    "    except OSError as error: \n",
    "        print(error)\n",
    "    clr_criterion  = VICRegLoss(lambda_param=1,mu_param=1,nu_param=1)\n",
    "    cor_criterion  = DiscoCorr(anti=False)\n",
    "    acr_criterion  = DiscoCorr(anti=True)\n",
    "\n",
    "    optimizer = optim.Adam(encoder.parameters())\n",
    "    loss_vals_training = np.zeros(n_epochs)\n",
    "    loss_vals_validation = np.zeros(n_epochs)\n",
    "\n",
    "    final_epoch = 0\n",
    "    l_val_best = 99999\n",
    "\n",
    "    for m in range(n_epochs):\n",
    "        print(\"Epoch %s\\n\" % m)\n",
    "        #torch.cuda.empty_cache()\n",
    "        final_epoch = m\n",
    "        lst = []\n",
    "        loss_val = []\n",
    "        loss_training = []\n",
    "        correct = []\n",
    "        tic = time.perf_counter()\n",
    "\n",
    "        particleTrainingDataSig, jetMassTrainingDataSig = sklearn.utils.shuffle(particleTrainingDataSig, jetMassTrainingDataSig)\n",
    "        particleTrainingDataBkg, jetMassTrainingDataBkg = sklearn.utils.shuffle(particleTrainingDataBkg, jetMassTrainingDataBkg)\n",
    "        particleValidationDataSig, jetMassValidationDataSig = sklearn.utils.shuffle(particleValidationDataSig,\n",
    "                                                                                    jetMassValidationDataSig)\n",
    "        particleValidationDataBkg, jetMassValidationDataBkg = sklearn.utils.shuffle(particleValidationDataBkg,\n",
    "                                                                                    jetMassValidationDataBkg)\n",
    "        \n",
    "        SigLen = len(particleTrainingDataSig)\n",
    "        BkgLen = len(particleTrainingDataBkg)\n",
    "        ratio = SigLen/BkgLen\n",
    "        \n",
    "        for i in tqdm(range(int(BkgLen/batchSize))): \n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            # Define training events\n",
    "            trainingvMassSig, SigSort = torch.FloatTensor(jetMassTrainingDataSig[round_even(i*batchSize*ratio):round_even((i+1)*batchSize*ratio)]).cuda().sort()\n",
    "            trainingvMassBkg, BkgSort = torch.FloatTensor(jetMassTrainingDataBkg[i*batchSize:(i+1)*batchSize]).cuda().sort()\n",
    "            trainingvSig = torch.FloatTensor(particleTrainingDataSig[round_even(i*batchSize*ratio):round_even((i+1)*batchSize*ratio)]).cuda()[SigSort]\n",
    "            trainingvBkg = torch.FloatTensor(particleTrainingDataBkg[i*batchSize:(i+1)*batchSize]).cuda()[BkgSort]\n",
    "            \n",
    "            #trainingvMassSig = torch.FloatTensor(jetMassTrainingDataSig[round_even(i*batchSize*ratio):round_even((i+1)*batchSize*ratio)]).cuda()\n",
    "            #trainingvMassBkg = torch.FloatTensor(jetMassTrainingDataBkg[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "            #trainingvSig = torch.FloatTensor(particleTrainingDataSig[round_even(i*batchSize*ratio):round_even((i+1)*batchSize*ratio)]).cuda()\n",
    "            #trainingvBkg = torch.FloatTensor(particleTrainingDataBkg[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "            \n",
    "            trainingv1 = torch.cat((trainingvSig[:int(len(trainingvSig)/2)], \n",
    "                                    trainingvBkg[:int(len(trainingvBkg)/2)]))\n",
    "            trainingv1_mass = torch.cat((trainingvMassSig[:int(len(trainingvSig)/2)], \n",
    "                                    trainingvMassBkg[:int(len(trainingvBkg)/2)]))\n",
    "            trainingv2 = torch.cat((trainingvSig[int(len(trainingvSig)/2):], \n",
    "                                    trainingvBkg[int(len(trainingvBkg)/2):]))\n",
    "            trainingv2_mass = torch.cat((trainingvMassSig[int(len(trainingvSig)/2):], \n",
    "                                    trainingvMassBkg[int(len(trainingvBkg)/2):]))\n",
    "            \n",
    "            # Calculate network output\n",
    "            out1 = encoder(trainingv1)\n",
    "            out2 = encoder(trainingv2)\n",
    "\n",
    "            #VICReg Loss\n",
    "            repr_loss, cov_loss, std_loss = clr_criterion(out1, out2)\n",
    "\n",
    "            l = weightrepr*repr_loss + weightcov*cov_loss + weightstd*std_loss\n",
    "            \n",
    "            \n",
    "            # For Clara: these can be commented out if not in use to make things run faster\n",
    "            # Anti-Correlation (actually correlation)\n",
    "            for dim in range(CorrDim): \n",
    "                l += weightCorr1*(dim+1)*acr_criterion(trainingv1_mass, out1[:,dim])\n",
    "                l += weightCorr1*(dim+1)*acr_criterion(trainingv2_mass, out2[:,dim])\n",
    "            \n",
    "            # Correlation for rest of dimensions (anti-correlation)\n",
    "            #for dim in range(1): \n",
    "            for dim in range(out1.shape[1]-CorrDim): \n",
    "                l += weightCorr2*(dim+1)*cor_criterion(out1[:,dim+CorrDim], trainingv1_mass)\n",
    "                l += weightCorr2*(dim+1)*cor_criterion(out2[:,dim+CorrDim], trainingv2_mass)\n",
    "            \n",
    "            \n",
    "            loss_training.append(l.item())\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            loss_string = \"Loss: %s\" % \"{0:.5f}\".format(l.item())\n",
    "            del trainingvSig, trainingvBkg, trainingv1_mass, trainingv2_mass, trainingv1, trainingv2, out1, out2\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        toc = time.perf_counter()\n",
    "        print(f\"Training done in {toc - tic:0.4f} seconds\")\n",
    "        tic = time.perf_counter()\n",
    "        out1_totSig = np.empty((0,n_Dim))\n",
    "        out1_totBkg = np.empty((0,n_Dim))\n",
    "\n",
    "        trainingv1_mass_totSig = []\n",
    "        trainingv1_mass_totBkg = []\n",
    "\n",
    "        \n",
    "        out_val_total_sig = []\n",
    "        out_val_total_bkg = []\n",
    "        out_val_mass_total_sig = []\n",
    "        out_val_mass_total_bkg = []\n",
    "        \n",
    "        SigLen = len(particleValidationDataSig)\n",
    "        BkgLen = len(particleValidationDataBkg)\n",
    "        ratio = SigLen/BkgLen\n",
    "        \n",
    "        for i in range(math.floor(BkgLen/batchSize)):\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            # Define validation events\n",
    "            trainingvMassSig_val, SigSort = torch.FloatTensor(jetMassValidationDataSig[round_even(ratio*i*batchSize):round_even(ratio*(i+1)*batchSize)]).cuda().sort()\n",
    "            trainingvMassBkg_val, BkgSort = torch.FloatTensor(jetMassValidationDataBkg[i*batchSize:(i+1)*batchSize]).cuda().sort()\n",
    "            trainingvSig_val = torch.FloatTensor(particleValidationDataSig[round_even(ratio*i*batchSize):round_even(ratio*(i+1)*batchSize)]).cuda()[SigSort]\n",
    "            trainingvBkg_val = torch.FloatTensor(particleValidationDataBkg[i*batchSize:(i+1)*batchSize]).cuda()[BkgSort]\n",
    "            \n",
    "            #trainingvMassSig_val = torch.FloatTensor(jetMassValidationDataSig[round_even(ratio*i*batchSize):round_even(ratio*(i+1)*batchSize)]).cuda()\n",
    "            #trainingvMassBkg_val = torch.FloatTensor(jetMassValidationDataBkg[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "            #trainingvSig_val = torch.FloatTensor(particleValidationDataSig[round_even(ratio*i*batchSize):round_even(ratio*(i+1)*batchSize)]).cuda()\n",
    "            #trainingvBkg_val = torch.FloatTensor(particleValidationDataBkg[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "            \n",
    "            \n",
    "            targetv_val = torch.FloatTensor(validationLabels[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "            \n",
    "            \n",
    "            trainingv1_val = torch.cat((trainingvSig_val[:int(len(trainingvSig_val)/2)], \n",
    "                                    trainingvBkg_val[:int(len(trainingvBkg_val)/2)]))\n",
    "            trainingv2_val = torch.cat((trainingvSig_val[int(len(trainingvSig_val)/2):], \n",
    "                                    trainingvBkg_val[int(len(trainingvBkg_val)/2):]))\n",
    "            \n",
    "            trainingv1_val_mass = torch.cat((trainingvMassSig_val[:int(len(trainingvSig_val)/2)], \n",
    "                                    trainingvMassBkg_val[:int(len(trainingvBkg_val)/2)]))\n",
    "            trainingv2_val_mass = torch.cat((trainingvMassSig_val[int(len(trainingvSig_val)/2):], \n",
    "                                    trainingvMassBkg_val[int(len(trainingvBkg_val)/2):]))\n",
    "            \n",
    "            \n",
    "            # For use in making plots later in epoch\n",
    "            if i == 0: \n",
    "                out_val_total_sig = encoder(trainingvSig_val).cpu().detach().numpy()\n",
    "                out_val_total_bkg= encoder(trainingvBkg_val).cpu().detach().numpy()\n",
    "                out_val_mass_total_sig = trainingvMassSig_val.cpu().detach().numpy()\n",
    "                out_val_mass_total_bkg = trainingvMassBkg_val.cpu().detach().numpy()\n",
    "                \n",
    "            else:\n",
    "                out_val_total_sig = np.concatenate((out_val_total_sig, encoder(trainingvSig_val).cpu().detach().numpy()))\n",
    "                out_val_total_bkg= np.concatenate((out_val_total_bkg, encoder(trainingvBkg_val).cpu().detach().numpy()))\n",
    "                out_val_mass_total_sig = np.concatenate((out_val_mass_total_sig, trainingvMassSig_val.cpu().detach().numpy()))\n",
    "                out_val_mass_total_bkg = np.concatenate((out_val_mass_total_bkg, trainingvMassBkg_val.cpu().detach().numpy()))\n",
    "\n",
    "            # VICReg Loss\n",
    "            out1_val = encoder(trainingv1_val)\n",
    "            out2_val = encoder(trainingv2_val)\n",
    "            repr_loss, cov_loss, std_loss = clr_criterion(out1_val, out2_val)\n",
    "\n",
    "            l_val = weightrepr*repr_loss + weightcov*cov_loss + weightstd*std_loss\n",
    "\n",
    "            # For Clara: these can be commented out if not in use to make things run faster\n",
    "            # AntiCorrelation\n",
    "            for dim in range(CorrDim): \n",
    "                l_val += weightCorr1*(dim+1)*acr_criterion(trainingv1_val_mass, out1_val[:,dim])\n",
    "                l_val += weightCorr1*(dim+1)*acr_criterion(trainingv2_val_mass, out2_val[:,dim])\n",
    "            \n",
    "            # Correlation for rest of dimensions\n",
    "            #for dim in range(1):\n",
    "            for dim in range(out1_val.shape[1]-CorrDim): \n",
    "                l_val += weightCorr2*(dim+1)*cor_criterion(out1_val[:,dim+CorrDim], trainingv1_val_mass)\n",
    "                l_val += weightCorr2*(dim+1)*cor_criterion(out2_val[:,dim+CorrDim], trainingv2_val_mass)\n",
    "    \n",
    "            \n",
    "            # Classical validation\n",
    "            loss_val.append(l_val.item())\n",
    "\n",
    "            del trainingvSig_val, trainingvBkg_val, trainingv1_val, trainingv2_val, out1_val, out2_val\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        print(np.array(out_val_total_sig).shape)\n",
    "        print(np.array(out_val_mass_total_sig).shape)\n",
    "        out_val_total_sig = np.array(out_val_total_sig).reshape(-1, n_Dim)\n",
    "        out_val_total_bkg = np.array(out_val_total_bkg).reshape(-1, n_Dim)\n",
    "        out_val_mass_total_sig = np.array(out_val_mass_total_sig).reshape(-1, 1)\n",
    "        out_val_mass_total_bkg = np.array(out_val_mass_total_bkg).reshape(-1, 1)\n",
    "        \n",
    "        fig,ax = plt.subplots()    \n",
    "        plt.clf()\n",
    "        fig, axs = plt.subplots(n_Dim,2, figsize=(10,n_Dim*10))\n",
    "\n",
    "        axs[0,0].text(0.05,2.8, loss_text, transform=ax.transAxes)\n",
    "\n",
    "        for dim in range(n_Dim): \n",
    "            outSig, massSig = out_val_total_sig[:, dim].copy(), out_val_mass_total_sig[:].copy()\n",
    "            outSig -= np.mean(outSig)\n",
    "            outSig /= np.std(outSig)\n",
    "            massSig -= np.mean(massSig)\n",
    "            massSig /= np.std(massSig)\n",
    "\n",
    "            outBkg, massBkg = out_val_total_bkg[:, dim].copy(), out_val_mass_total_bkg[:].copy()\n",
    "            outBkg -= np.mean(outBkg)\n",
    "            outBkg /= np.std(outBkg)\n",
    "            massBkg -= np.mean(massBkg)\n",
    "            massBkg /= np.std(massBkg)\n",
    "\n",
    "            outSig = outSig.reshape(-1)\n",
    "            outBkg = outBkg.reshape(-1)\n",
    "            massSig = massSig.reshape(-1)\n",
    "            massBkg = massBkg.reshape(-1)\n",
    "            \n",
    "            axs[dim,0].text(0.8,1.03,f\"Z' Corr:  {np.corrcoef(outSig, massSig)[0,1] : .4f}\", transform=axs[dim,0].transAxes)\n",
    "            axs[dim,0].hist2d(outSig, out_val_mass_total_sig.reshape(-1), bins=30, )\n",
    "            axs[dim,1].text(0.8,1.03,f\"QCD Corr: {np.corrcoef(outBkg, massBkg)[0,1] : .4f}\", transform=axs[dim,1].transAxes)\n",
    "            axs[dim,1].hist2d(outBkg, out_val_mass_total_bkg.reshape(-1), bins=30, )\n",
    "            axs[dim,0].set_xlim([-3.,3.])\n",
    "            axs[dim,1].set_xlim([-3.,3.])\n",
    "            axs[dim,0].set_xlabel(f'Dimension {dim} output')\n",
    "            axs[dim,1].set_xlabel(f'Dimension {dim} output')\n",
    "            axs[dim,0].set_ylabel('Jet mass (GeV)')\n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.savefig(outdir+\"/\"+modelName+f\"_contrastivefigIN_trainingDataset_epoch{m}.jpg\")\n",
    "\n",
    "        try: \n",
    "            label_str = [\"latent var %s\"%str(i) for i in range(n_Dim)]\n",
    "            label_str.append(\"mass\")\n",
    "            fig = corner.corner(np.concatenate((out_val_total_sig, out_val_mass_total_sig.reshape(-1, 1)), axis=1), color='red', labels=label_str)\n",
    "            corner.corner(np.concatenate((out_val_total_bkg, out_val_mass_total_bkg.reshape(-1, 1)), axis=1), fig=fig, color='blue', labels=label_str)\n",
    "            fig.savefig('%s/CornerPlot_%s.jpg'%(outdir,modelName))\n",
    "        except: \n",
    "            # Can happen at beginning of trainings\n",
    "            print('corner plot problems - might not plot? but also might plot?')\n",
    "\n",
    "        # Calculate train/val loss over epoch\n",
    "        toc = time.perf_counter()\n",
    "        print(f\"Evaluation done in {toc - tic:0.4f} seconds\")\n",
    "        l_val = np.mean(np.array(loss_val))\n",
    "\n",
    "        print('\\nValidation Loss: ', l_val)\n",
    "\n",
    "        l_training = np.mean(np.array(loss_training))\n",
    "        print('Training Loss: ', l_training)\n",
    "\n",
    "        torch.save(encoder.state_dict(), '%s/encoder_%s_last.pth'%(outdir,modelName))\n",
    "        if l_val < l_val_best:\n",
    "            print(\"new best model\")\n",
    "            l_val_best = l_val\n",
    "            torch.save(encoder.state_dict(), '%s/encoder_%s_best.pth'%(outdir,modelName))\n",
    "\n",
    "        loss_vals_training[m] = l_training\n",
    "        loss_vals_validation[m] = l_val\n",
    "\n",
    "        # Early stopping\n",
    "        if m > 8 and all(loss_vals_validation[max(0, m - 8):m] > min(np.append(loss_vals_validation[0:max(0, m - 8)], 200))):\n",
    "            print('Early Stopping...')\n",
    "            print(loss_vals_training, '\\n', np.diff(loss_vals_training))\n",
    "            break\n",
    "            \n",
    "    print(loss_vals_training, '\\n', np.diff(loss_vals_training))\n",
    "    \n",
    "    print('DONE with ENCODER training')\n",
    "    return encoder\n",
    "\n",
    "def train_classifier(classifier, encoder, batchSize, n_Dim, CorrDim, n_epochs, modelName, outdir, \n",
    "                    particleTrainingData, particleValidationData, trainingLabels, jetMassTrainingData, jetMassValidationData):   \n",
    "    \n",
    "    # Separating signal and bkg arrays\n",
    "    particleTrainingDataSig = particleTrainingData[trainingLabels[:,0].astype(bool)]\n",
    "    particleTrainingDataBkg = particleTrainingData[trainingLabels[:,1].astype(bool)]\n",
    "    particleValidationDataSig = particleValidationData[validationLabels[:,0].astype(bool)]\n",
    "    particleValidationDataBkg = particleValidationData[validationLabels[:,1].astype(bool)]\n",
    "    particleTrainingLabelSig = trainingLabels[trainingLabels[:,0].astype(bool)]\n",
    "    particleTrainingLabelBkg = trainingLabels[trainingLabels[:,1].astype(bool)]\n",
    "\n",
    "    # Jet mass for correlation\n",
    "    jetMassTrainingDataSig = jetMassTrainingData[trainingLabels[:,0].astype(bool)]\n",
    "    jetMassTrainingDataBkg = jetMassTrainingData[trainingLabels[:,1].astype(bool)]\n",
    "    jetMassValidationDataSig = jetMassValidationData[validationLabels[:,0].astype(bool)]\n",
    "    jetMassValidationDataBkg = jetMassValidationData[validationLabels[:,1].astype(bool)]\n",
    "    \n",
    "    \n",
    "    loss = nn.BCELoss(reduction='mean')  \n",
    "    optimizer = optim.Adam(classifier.parameters(), lr = 0.001)\n",
    "\n",
    "    loss_vals_training = np.zeros(n_epochs)\n",
    "    loss_vals_validation = np.zeros(n_epochs)\n",
    "    acc_vals_training = np.zeros(n_epochs)\n",
    "    acc_vals_validation = np.zeros(n_epochs)\n",
    "    \n",
    "    final_epoch = 0\n",
    "    l_val_best = 99999\n",
    "    \n",
    "    for m in range(n_epochs):\n",
    "        print(\"Epoch %s\\n\" % m)\n",
    "        tic = time.perf_counter()\n",
    "        final_epoch = m\n",
    "        lst = []\n",
    "        loss_val = []\n",
    "        loss_training = []\n",
    "        correct = []\n",
    "        tic = time.perf_counter()\n",
    "\n",
    "        totaldiv2 = min(len(particleTrainingDataSig), len(particleTrainingDataBkg))\n",
    "        for i in tqdm(range(int(len(particleTrainingData)/batchSize))): \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            ######### train classifier #########\n",
    "            trainingv = torch.FloatTensor(particleTrainingData[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "            targetv = torch.FloatTensor(trainingLabels[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "            out = classifier(encoder(trainingv)[:, CorrDim:])\n",
    "            l = loss(out, targetv)\n",
    "\n",
    "            loss_training.append(l.item())\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_string = \"Loss: %s\" % \"{0:.5f}\".format(l.item())\n",
    "            del trainingv, targetv, out\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        toc = time.perf_counter()\n",
    "        print(f\"Training done in {toc - tic:0.4f} seconds\")\n",
    "        tic = time.perf_counter()\n",
    "\n",
    "        for i in range(int(len(validationLabels)/batchSize)): \n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            # Classifier \n",
    "            targetv_val = torch.FloatTensor(validationLabels[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "            trainingv_val = torch.FloatTensor(particleValidationData[i*batchSize:(i+1)*batchSize]).cuda()\n",
    "            out = classifier(encoder(trainingv_val)[:, CorrDim:])\n",
    "\n",
    "            l_val = loss(out, targetv_val)\n",
    "            lst.append(out.cpu().data.numpy())\n",
    "            loss_val.append(l_val.item())\n",
    "            correct.append(targetv_val.cpu())\n",
    "\n",
    "            del trainingv_val, targetv_val, out\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "        toc = time.perf_counter()\n",
    "        print(f\"Evaluation done in {toc - tic:0.4f} seconds\")\n",
    "        l_val = np.mean(np.array(loss_val))\n",
    "\n",
    "        print('\\nValidation Loss: ', l_val)\n",
    "\n",
    "        l_training = np.mean(np.array(loss_training))\n",
    "        print('Training Loss: ', l_training)\n",
    "        \n",
    "        predicted = np.concatenate(lst)\n",
    "        val_targetv = np.concatenate(correct)\n",
    "        acc_vals_validation[m] = accuracy_score(val_targetv[:,0],predicted[:,0]>0.5)\n",
    "        print(\"Validation Accuracy: \", acc_vals_validation[m])\n",
    "        \n",
    "        torch.save(classifier.state_dict(), '%s/classifier_%s_last.pth'%(outdir,modelName))\n",
    "        if l_val < l_val_best:\n",
    "            print(\"new best model\")\n",
    "            l_val_best = l_val\n",
    "            torch.save(classifier.state_dict(), '%s/classifier_%s_best.pth'%(outdir,modelName))\n",
    "        loss_vals_training[m] = l_training\n",
    "        loss_vals_validation[m] = l_val\n",
    "        if m > 8 and all(loss_vals_validation[max(0, m - 8):m] > min(np.append(loss_vals_validation[0:max(0, m - 8)], 200))):\n",
    "            print('Early Stopping...')\n",
    "            print(loss_vals_training, '\\n', np.diff(loss_vals_training))\n",
    "            break\n",
    "\n",
    "    print(loss_vals_training, '\\n', np.diff(loss_vals_training))\n",
    "    print('DONE with CLASSIFIER training')\n",
    "    \n",
    "    return classifier\n",
    "\n",
    "import matplotlib.ticker as plticker\n",
    "def eval_classifier(classifier, encoder, loss_params_text, outdir, batchSize, CorrDim):  \n",
    "    testv = torch.FloatTensor(particleTestData).cuda()\n",
    "    predictions = []\n",
    "    batchSize = 1327\n",
    "    for i in tqdm(range(int(len(testv)/batchSize))):\n",
    "        print(testv[i*batchSize:(i+1)*batchSize].shape)\n",
    "        print(encoder(testv[i*batchSize:(i+1)*batchSize]).shape)\n",
    "        predictions.append(classifier(encoder(testv[i*batchSize:(i+1)*batchSize])[:, CorrDim:]).cpu().detach().numpy())\n",
    "    print(np.array(predictions).shape)\n",
    "    predictions = np.array(predictions).reshape((-1, 2))\n",
    "    \n",
    "    testData = totalData[trainingDataLength + validationDataLength:, ]\n",
    "    print(predictions)\n",
    "    fpr, tpr, threshold = roc_curve(np.array(testLabels)[:,1].reshape(-1), np.array(predictions)[:,1].reshape(-1))\n",
    "    np.savez(outdir+\"/rocvals\",fpr=fpr,tpr=tpr)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, lw=2.5, label=\"{}, AUC = {:.1f} %\".format('ZprimeAtoqq IN',auc(fpr,tpr)*100))\n",
    "    plt.title('ROC Curve')\n",
    "    plt.xlabel('FPR')\n",
    "    plt.ylabel('TPR')\n",
    "    plt.legend()\n",
    "    plt.savefig('%s/%s_model_ROC.jpg'%(outdir,modelName))\n",
    " \n",
    "    sculpt_vars = ['jet_eta', \"jet_phi\",\"jet_EhadOverEem\",\"jet_mass\", 'jet_pT', 'jet_sdmass']\n",
    "    for i in range(len(sculpt_vars)):\n",
    "        \n",
    "        # Calculate sculpt_var distribution after cuts\n",
    "        hist, edges = np.histogram(predictions[testLabels[:,1] == 1][:,1], bins=np.linspace(0.,1.,100),density=True)\n",
    "        #hist, edges = np.histogram(outputs[y_torch[:,1].cpu().detach().numpy()==1][:,1].cpu().detach().numpy(), bins=np.linspace(0.,1.,100),density=True)\n",
    "        cdf = np.cumsum(hist)*(edges[1]-edges[0])\n",
    "\n",
    "        pctls = [0.,0.25,0.5,0.7,0.9,0.95,0.99]\n",
    "        cuts = np.searchsorted(cdf,pctls)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "        m_torch = testData[:, i]\n",
    "        qcd_idxs = testLabels[:,1].astype(bool)\n",
    "\n",
    "        qcd_inclusive, _ = np.histogram(m_torch[(qcd_idxs)], density=True)\n",
    "\n",
    "        for c,p in zip(cuts,pctls):\n",
    "            passing_idxs = predictions[:,1] > edges[c]\n",
    "            hist, bin_edges = np.histogram(\n",
    "                m_torch[(qcd_idxs&passing_idxs)], \n",
    "            )\n",
    "            N_passing = float(np.sum(hist))\n",
    "            qcd_passing = np.divide(hist,[N_passing])\n",
    "            jsd = scipy.spatial.distance.jensenshannon(qcd_passing, qcd_inclusive)\n",
    "\n",
    "            bins_centers = 0.5*(bin_edges[1:]+bin_edges[:-1])\n",
    "            ax.plot(\n",
    "                bins_centers, \n",
    "                qcd_passing,\n",
    "                label = f\"{(1-p)*100:.0f}% ({int(N_passing)}) JSD={0 if jsd is np.nan else jsd:.2f}\"\n",
    "            )\n",
    "\n",
    "        \n",
    "        if sculpt_vars[i] == 'jet_sdmass':\n",
    "            ax.set_xlabel(r'$\\mathrm{Jet\\ m_{SD}\\ (GeV)}$', ha='right', x=1.0, fontsize=16)\n",
    "        elif sculpt_vars[i] == 'jet_pT':\n",
    "            ax.set_xlabel(r'$\\mathrm{Jet\\ pT\\ (GeV)}$', ha='right', x=1.0, fontsize=16)\n",
    "        else: \n",
    "            ax.set_xlabel(sculpt_vars[i], ha='right', x=1.0, fontsize=16)\n",
    "        ax.set_ylabel(r'Normalized scale ({})'.format('QCD'), ha='right', y=1.0, fontsize=16)\n",
    "        import matplotlib.ticker as plticker\n",
    "        ax.xaxis.set_major_locator(plticker.MultipleLocator(base=20))\n",
    "        ax.xaxis.set_minor_locator(plticker.MultipleLocator(base=10))\n",
    "        ax.yaxis.set_minor_locator(plticker.AutoMinorLocator(5))\n",
    "        \n",
    "        pt_range = [400., 1000.]\n",
    "        mass_range = [40., 250.]\n",
    "        \n",
    "        if sculpt_vars[i] == 'jet_sdmass':\n",
    "            ax.set_xlim(mass_range[0], mass_range[1])\n",
    "            ax.xaxis.set_major_locator(plticker.MultipleLocator(base=20))\n",
    "            ax.xaxis.set_minor_locator(plticker.MultipleLocator(base=10))\n",
    "            ax.yaxis.set_minor_locator(plticker.AutoMinorLocator(5))\n",
    "        elif sculpt_vars[i] == 'jet_pT':\n",
    "            ax.set_xlim(pt_range[0], pt_range[1])\n",
    "            ax.xaxis.set_major_locator(plticker.MultipleLocator(base=50))\n",
    "            ax.xaxis.set_minor_locator(plticker.MultipleLocator(base=25))\n",
    "            ax.yaxis.set_minor_locator(plticker.AutoMinorLocator(5))\n",
    "        else: \n",
    "            ax.xaxis.set_major_locator(plticker.MultipleLocator(base=20))\n",
    "            ax.xaxis.set_minor_locator(plticker.MultipleLocator(base=10))\n",
    "            ax.yaxis.set_minor_locator(plticker.AutoMinorLocator(5))\n",
    "        \n",
    "        ax.set_ylim(0, 0.30)\n",
    "        ax.tick_params(direction='in', axis='both', which='major', labelsize=15, length=12)#, labelleft=False )\n",
    "        ax.tick_params(direction='in', axis='both', which='minor' , length=6)\n",
    "        ax.xaxis.set_ticks_position('both')\n",
    "        ax.yaxis.set_ticks_position('both')    \n",
    "        #ax.grid(which='minor', alpha=0.5, axis='y', linestyle='dotted')\n",
    "        ax.grid(which='major', alpha=0.9, linestyle='dotted')\n",
    "        plt.legend(loc=\"best\", fontsize=13)\n",
    "        \n",
    "        \n",
    "        leg = ax.text(0.03, 0.88, \"\"+str(int(round((min(pt_range)))))+\" $\\mathrm{<\\ Jet\\ p_T\\ <}$ \"+str(int(round((max(pt_range)))))+\" GeV\" \\\n",
    "              + \"\\n \"+str(int(round((min(mass_range)))))+\" $\\mathrm{<\\ Jet\\ m_{SD}\\ <}$ \"+str(int(round((max(mass_range)))))+\" GeV\"\n",
    "                      + \"\\n Sculpted Sample\"\n",
    "                  , fontsize=13, transform=ax.transAxes) #borderpad=1, frameon=False, loc='upper left', fontsize=16,          )\n",
    "        #leg._legend_box.align = \"left\"\n",
    "        \n",
    "        #ax.set_xlabel(sculpt_vars[i])\n",
    "        #ax.set_ylabel(\"a.u.\")\n",
    "        #ax.text(0.05,1.03,\"QCD jets\", transform=ax.transAxes)\n",
    "        ax.set_title(loss_text + ' Z\\'qq'  , transform=ax.transAxes, fontsize=16)\n",
    "        plt.savefig(outdir+\"/sculptingQCD_%s.png\"%(sculpt_vars[i]))\n",
    "        #plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7836610",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Contrastive Training\n",
    "\n",
    "batchSize = 1024\n",
    "n_Dim = 4\n",
    "n_epochs = 200\n",
    "CorrDim = 1\n",
    "label='Contrastive'\n",
    "\n",
    "\n",
    "weightrepr = 1\n",
    "weightcov = 1 #(most useful)\n",
    "weightstd = 1\n",
    "weightCorr1 = 1 #(most useful)\n",
    "weightCorr2 = 0 #(not really useful but could explore)\n",
    "loss_text = 'lambda_cov=%s, lambdacorr1=%s, lambdacorr2=%s'%(weightcov, weightCorr1, weightCorr2)\n",
    "loss_text = \"Contrastive Training\"\n",
    "\n",
    "\n",
    "modelName = \"IN_FlatSamples_flatratio_DiscoCorr%s_WeightCov%s_DiscoAntiCorr%s_FixedVicReg\"%(weightcov, weightCorr1, weightCorr2) + label\n",
    "outdir = 'Plots_Boost/' + modelName #everything will output here\n",
    "\n",
    "try: \n",
    "    os.mkdir(outdir) \n",
    "except OSError as error: \n",
    "    print(error)\n",
    "    \n",
    "#encoder = DNN(n_Dim)\n",
    "\n",
    "encoder = GraphNetnoSV(particlesPostCut, n_Dim, entriesPerParticle, 40,\n",
    "                      De=80,\n",
    "                      Do=20, softmax=False, attention_flag=True)\n",
    "\n",
    "classifier = Linear(n_Dim-CorrDim, 2)\n",
    "\n",
    "encoder = train_encoder(encoder, batchSize, n_Dim, CorrDim, n_epochs, modelName, outdir, \n",
    "            particleTrainingData, particleValidationData, trainingLabels, jetMassTrainingData, jetMassValidationData,\n",
    "            weightrepr, weightcov, weightstd, weightCorr1, weightCorr2)\n",
    "\n",
    "encoder.load_state_dict(torch.load(outdir+'/encoder_' + modelName +'_best.pth'))\n",
    "classifier = train_classifier(classifier, encoder, batchSize, n_Dim, CorrDim, n_epochs, modelName, outdir, \n",
    "            particleTrainingData, particleValidationData, trainingLabels, jetMassTrainingData, jetMassValidationData)\n",
    "\n",
    "classifier.load_state_dict(torch.load(outdir+'/classifier_' + modelName +'_best.pth'))\n",
    "\n",
    "eval_classifier(classifier, encoder, loss_text, outdir, batchSize, CorrDim)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343446b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Phil: Useful stuff ends here. Below stuff might be a big buggy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e278bd63",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Supervised Training\n",
    "batchSize = 1024\n",
    "n_Dim = 2\n",
    "n_epochs = 200\n",
    "CorrDim = 0\n",
    "label='Supervised'\n",
    "modelName = \"INv2attention_FlatSamples_flatratio_\" + label\n",
    "outdir = 'Plots_Boost/' + modelName #everything will output here\n",
    "\n",
    "try: \n",
    "    os.mkdir(outdir) \n",
    "except OSError as error: \n",
    "    print(error)\n",
    "\n",
    "loss_text = \"Supervised Training\"\n",
    "\n",
    "\n",
    "def encoder(x):\n",
    "    return(x)\n",
    "\n",
    "classifier = GraphNetnoSV(particlesPostCut, n_Dim, entriesPerParticle, 40,\n",
    "                      De=80,\n",
    "                       Do=20, softmax=True, attention_flag=True)\n",
    "\n",
    "#classifier = DNN(2, softmaxFlag=True)\n",
    "#classifier = MLP(200, n_Dim)\n",
    "\n",
    "classifier = train_classifier(classifier, encoder, batchSize, n_Dim, CorrDim, n_epochs, modelName, outdir, \n",
    "            particleTrainingData, particleValidationData, trainingLabels, jetMassTrainingData, jetMassValidationData)\n",
    "\n",
    "classifier.load_state_dict(torch.load(outdir+'/classifier_' + modelName +'_best.pth'))\n",
    "\n",
    "eval_classifier(classifier, encoder, loss_text, outdir, batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ca79c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_classifier(classifier, encoder, loss_text, outdir, batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cc37cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Combined ROC plots\n",
    "\n",
    "plotdir = 'Plots_Boost/'\n",
    "rocvals = 'rocvals.npz'\n",
    "pt_range = [400., 1000.]\n",
    "mass_range = [40., 250.]\n",
    "    \n",
    "f, ax = plt.subplots(figsize=(10, 10))\n",
    "for training, name in zip(['INv2attention_FlatSamples_flatratio_Contrastive', 'INv2_FlatSamples_flatratio_Supervised', 'INv2attention_FlatSamples_flatratio_Supervised'], \n",
    "                          ['Contrastive IN + MLP','IN', 'IN+Attention', 'MoDe0', 'N2']): \n",
    "    rocdata = np.load(plotdir + training + '/' + rocvals)   \n",
    "    tpr = rocdata['tpr']\n",
    "    fpr = rocdata['fpr']\n",
    "    ax.plot(tpr, fpr, lw=2.5, label=\"{}, AUC = {:.1f}%\".format(name,auc(fpr,tpr)*100))\n",
    "    print(\"{}, AUC={}%\".format(name, auc(fpr,tpr)*100))\n",
    "    \n",
    "ax.set_xlim(0,1)\n",
    "ax.set_ylim(0.001,1)\n",
    "xlab = '{} \\\\rightarrow {}'.format('Z\\'','qq') \n",
    "ax.set_xlabel(r'Tagging efficiency ($\\mathrm{}$)'.format('{'+xlab+'}'), ha='right', x=1.0, fontsize=16)\n",
    "ylab = '{}'.format('QCD') \n",
    "ax.set_ylabel(r'Mistagging rate ($\\mathrm{}$)'.format('{'+ylab+'}'), ha='right', y=1.0, fontsize=16)\n",
    "\n",
    "import matplotlib.ticker as plticker\n",
    "ax.xaxis.set_major_locator(plticker.MultipleLocator(base=0.1))\n",
    "ax.xaxis.set_minor_locator(plticker.MultipleLocator(base=0.02))\n",
    "ax.tick_params(direction='in', axis='both', which='major', labelsize=15, length=12 )\n",
    "ax.tick_params(direction='in', axis='both', which='minor' , length=6)\n",
    "ax.xaxis.set_ticks_position('both')\n",
    "ax.yaxis.set_ticks_position('both')    \n",
    "#ax.semilogy()\n",
    "ax.grid(which='minor', alpha=0.5, axis='y', linestyle='dotted')\n",
    "ax.grid(which='major', alpha=0.9, linestyle='dotted')\n",
    "leg = ax.legend(borderpad=1, frameon=False, loc=2, fontsize=15,\n",
    "    title = \"\"+str(int(round((min(pt_range)))))+\" $\\mathrm{<\\ Jet\\ p_T\\ <}$ \"+str(int(round((max(pt_range)))))+\" GeV\" \\\n",
    "              + \"\\n \"+str(int(round((min(mass_range)))))+\" $\\mathrm{<\\ Jet\\ m_{SD}\\ <}$ \"+str(int(round((max(mass_range)))))+\" GeV\"\n",
    "                      + \"\\n Sculpted Sample\")\n",
    "\n",
    "leg._legend_box.align = \"left\"\n",
    "plt.setp(leg.get_title(),fontsize='x-large')\n",
    "plt.show()\n",
    "\n",
    "f, ax = plt.subplots(figsize=(10, 10))\n",
    "for training, name in zip([\"INv2attention_FlatSamples_flatratio_Contrastive\"'DNN_FlatSamples_flatratio_Supervised', 'DNN_FlatSamples_flatratio_Contrastive', 'DNN_Mode0_flatratio', 'N2'], \n",
    "                          ['Supervised', 'Contrastive', 'MoDe0', 'N2']): \n",
    "    rocdata = np.load(plotdir + training + '/' + rocvals)   \n",
    "    tpr = rocdata['tpr']\n",
    "    fpr = rocdata['fpr']\n",
    "    ax.plot(tpr, fpr, lw=2.5, label=\"{}, AUC = {:.1f}%\".format(name,auc(fpr,tpr)*100))\n",
    "    print(\"{}, AUC={}%\".format(name, auc(fpr,tpr)*100))\n",
    "    \n",
    "ax.set_xlim(0,1)\n",
    "ax.set_ylim(0.001,1)\n",
    "xlab = '{} \\\\rightarrow {}'.format('Z\\'','qq') \n",
    "ax.set_xlabel(r'Tagging efficiency ($\\mathrm{}$)'.format('{'+xlab+'}'), ha='right', x=1.0, fontsize=16)\n",
    "ylab = '{}'.format('QCD') \n",
    "ax.set_ylabel(r'Mistagging rate ($\\mathrm{}$)'.format('{'+ylab+'}'), ha='right', y=1.0, fontsize=16)\n",
    "\n",
    "import matplotlib.ticker as plticker\n",
    "ax.xaxis.set_major_locator(plticker.MultipleLocator(base=0.1))\n",
    "ax.xaxis.set_minor_locator(plticker.MultipleLocator(base=0.02))\n",
    "ax.tick_params(direction='in', axis='both', which='major', labelsize=15, length=12 )\n",
    "ax.tick_params(direction='in', axis='both', which='minor' , length=6)\n",
    "ax.xaxis.set_ticks_position('both')\n",
    "ax.yaxis.set_ticks_position('both')    \n",
    "#ax.semilogy()\n",
    "ax.grid(which='minor', alpha=0.5, axis='y', linestyle='dotted')\n",
    "ax.grid(which='major', alpha=0.9, linestyle='dotted')\n",
    "leg = ax.legend(borderpad=1, frameon=False, loc=2, fontsize=15,\n",
    "    title = \"\"+str(int(round((min(pt_range)))))+\" $\\mathrm{<\\ Jet\\ p_T\\ <}$ \"+str(int(round((max(pt_range)))))+\" GeV\" \\\n",
    "              + \"\\n \"+str(int(round((min(mass_range)))))+\" $\\mathrm{<\\ Jet\\ m_{SD}\\ <}$ \"+str(int(round((max(mass_range)))))+\" GeV\"\n",
    "                      + \"\\n Sculpted Sample\")\n",
    "\n",
    "leg._legend_box.align = \"left\"\n",
    "plt.setp(leg.get_title(),fontsize='x-large')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fadd61f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IN_torch",
   "language": "python",
   "name": "in_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
